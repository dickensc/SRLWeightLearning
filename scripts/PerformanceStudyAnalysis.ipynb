{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "datasets = {'Jester': {'n_folds': 8,\n",
    "                       'evaluation_methods': ['MSE', 'AUROC']\n",
    "                      }, \n",
    "            'Epinions': {'n_folds': 8,\n",
    "                         'evaluation_methods': ['F1', 'AUROC']\n",
    "                        },\n",
    "            'Cora': {'n_folds': 8, \n",
    "                     'evaluation_methods': ['F1', 'Accuracy']\n",
    "                    },\n",
    "            'Citeseer': {'n_folds': 8, \n",
    "                         'evaluation_methods': ['F1', 'Accuracy']\n",
    "                        },\n",
    "            'LastFM': {'n_folds': 5, \n",
    "                       'evaluation_methods': ['MSE', 'AUROC']\n",
    "                      }\n",
    "           }\n",
    "\n",
    "# weight_learning_methods = [\"BOWLSS\", \"BOWLOS\", \"LME\", \"MLE\",\n",
    "#            \"MPLE\", \"RGS\", \"CRGS\", \"HB\"]\n",
    "\n",
    "weight_learning_methods = {\"psl\": [\"BOWLSS\", \"BOWLOS\", \"UNIFORM\"],\n",
    "                           \"tuffy\": [\"UNIFORM\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/charles/SRLWeightLearning/scripts\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset=\"epinions\"\n",
    "# wl_method=\"UNIFORM\"\n",
    "# evaluation_metric=\"Discrete\"\n",
    "# fold=0\n",
    "# predicate=\"trusts\"\n",
    "\n",
    "# dataset=\"citeseer\"\n",
    "# wl_method=\"UNIFORM\"\n",
    "# evaluation_metric=\"Categorical\"\n",
    "# fold=0\n",
    "# predicate=\"hasCat\"\n",
    "\n",
    "dataset=\"jester\"\n",
    "wl_method=\"UNIFORM\"\n",
    "evaluation_metric=\"Continuous\"\n",
    "fold=0\n",
    "predicate=\"rating\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_truth_frame(dataset, fold, predicate):\n",
    "    # truth dataframe \n",
    "    truth_path = \"../psl-examples/{}/data/{}/{}/eval/{}_truth.txt\".format(dataset, dataset, fold, predicate)\n",
    "    truth_df = pd.read_csv(truth_path, sep='\\t', header=None)\n",
    "    \n",
    "    # clean up column names and set multi-index for predicate\n",
    "    arg_columns = ['arg_' + str(col) for col in truth_df.columns[:-1]]\n",
    "    value_column = ['val']\n",
    "    truth_df.columns = arg_columns + value_column\n",
    "    truth_df = truth_df.astype({col: int for col in arg_columns})\n",
    "    truth_df = truth_df.set_index(arg_columns)\n",
    "    \n",
    "    return truth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psl_experiment_prediction_frame(dataset, wl_method, evaluation_metric, fold, predicate):\n",
    "    \n",
    "    # predicted dataframe \n",
    "    predicted_path = \"../results/weightlearning/psl/performance_study/{}/{}/{}/{}/inferred-predicates/{}.txt\".format(\n",
    "        dataset, wl_method, evaluation_metric, fold, predicate.upper())\n",
    "    predicted_df = pd.read_csv(predicted_path, sep='\\t', header=None)\n",
    "\n",
    "    # clean up column names and set multi-index for predicate\n",
    "    arg_columns = ['arg_' + str(col) for col in predicted_df.columns[:-1]]\n",
    "    value_column = ['val']\n",
    "    predicted_df.columns = arg_columns + value_column\n",
    "    predicted_df = predicted_df.astype({col: int for col in arg_columns})\n",
    "    predicted_df = predicted_df.set_index(arg_columns)\n",
    "\n",
    "    # truth dataframe \n",
    "    truth_df = load_truth_frame(dataset, fold, predicate)\n",
    "\n",
    "    # Join predicted_df and truth_df on the arguments\n",
    "    prediction_frame = truth_df.join(predicted_df, how=\"left\",\n",
    "                                     lsuffix='_truth', rsuffix='_predicted').fillna(0)\n",
    "\n",
    "    return prediction_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filename):\n",
    "    output = []\n",
    "\n",
    "    with open(filename, 'r') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            output.append(line)\n",
    "\n",
    "    return output\n",
    "\n",
    "def load_tuffy(tuffy_dir):\n",
    "    results_path = os.path.join(tuffy_dir, 'inferred-predicates.txt')\n",
    "    results_tmp = load_file(results_path)\n",
    "    results = []\n",
    "    \n",
    "    for result in results_tmp:\n",
    "        if len(result) == 1:\n",
    "            # then we did not run in marginal mode, i.e. outputs in this file are all \"true\" or 1\n",
    "            predicate = result[0][result[0].find(\"(\")+1:result[0].find(\")\")].replace(' ', '').split(',')\n",
    "            predicate.append(1.0)\n",
    "            results.append(predicate)\n",
    "        else:\n",
    "            # we ran this experiment in marginal mode, i.e., the marginal probability precedes the ground atom\n",
    "            predicate = result[1][result[1].find(\"(\")+1:result[1].find(\")\")].replace(' ', '').split(',')\n",
    "            predicate.append(result[0])\n",
    "            results.append(predicate)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_tuffy_experiment_prediction_frame(dataset, wl_method, evaluation_metric, fold, predicate):\n",
    "    \n",
    "    # read inferred and truth data\n",
    "    tuffy_experiment_directory = \"../results/weightlearning/tuffy/performance_study/{}/{}/{}/{}\".format(\n",
    "        dataset, wl_method, evaluation_metric, fold)\n",
    "\n",
    "    results = load_tuffy(tuffy_experiment_directory)\n",
    "    predicted_df = pd.DataFrame(results)\n",
    "\n",
    "    # clean up column names and set multi-index for predicate\n",
    "    arg_columns = ['arg_' + str(col) for col in predicted_df.columns[:-1]]\n",
    "    value_column = ['val']\n",
    "    predicted_df.columns = arg_columns + value_column\n",
    "    predicted_df = predicted_df.astype({col: int for col in arg_columns})\n",
    "    predicted_df = predicted_df.set_index(arg_columns)\n",
    "\n",
    "    # truth dataframe \n",
    "    truth_df = load_truth_frame(dataset, fold, predicate)\n",
    "\n",
    "    # Join predicted_df and truth_df on the arguments\n",
    "    prediction_frame = truth_df.join(predicted_df, how=\"left\",\n",
    "                                     lsuffix='_truth', rsuffix='_predicted').fillna(0)\n",
    "\n",
    "    return prediction_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frame = get_psl_experiment_prediction_frame(dataset, wl_method, evaluation_metric, fold, predicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mse(predictions, truths):\n",
    "    mse = {\"mse\": mean_squared_error(truths, predictions)}\n",
    "    return mse\n",
    "\n",
    "\n",
    "def evaluate_accuracy(predictions, truths):\n",
    "    accuracy = {\"accuracy\": accuracy_score(truths, predictions)}\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def evaluate_f1(predictions, truths, threshold=0.5):\n",
    "    b_predictions = predictions > threshold\n",
    "    b_truths = truths > threshold\n",
    "    \n",
    "    f_1 = {\"positive_class_f1\": f1_score(b_truths, b_predictions, pos_label=True),\n",
    "           \"negative_class_f1\": f1_score(b_truths, b_predictions, pos_label=False)}\n",
    "\n",
    "    return f_1\n",
    "\n",
    "def evaluate_roc_auc_score(predictions, truths, threshold=0.5):\n",
    "    relevant = truths > threshold\n",
    "    auroc = {\"auroc\": roc_auc_score(relevant, predictions)}\n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_psl_experiment_performance_results(dataset, wl_method, metric, n_folds):\n",
    "    # for each metric in the evaluator type make a series with the index representing the fold and value the outcome\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_psl_performance_results():\n",
    "    # in results/weightlearning/psl/performance_study write \n",
    "    # a performance.csv file with columns \n",
    "    # Dataset | WL_Method | Evaluation_Method | Mean | Standard_Deviation\n",
    "    \n",
    "    # we are going to overwrite the file with all the most up to date information\n",
    "    \n",
    "    psl_performance_frame = pd.DataFrame(columns=['Dataset', 'Wl_Method', 'Evaluation_Method', 'Mean', 'Standard_Deviation'])\n",
    "    \n",
    "    # find all the files that are in the results directory\n",
    "    path = '../results/weightlearning/psl/performance_study'\n",
    "    datasets = [dataset for dataset in os.listdir(path) if os.path.isdir(os.path.join(path, dataset))]\n",
    "    \n",
    "    # iterate over all datasets adding the results to the psl_performance_frame\n",
    "    for dataset in datasets:\n",
    "        # find all the files that are in the results directory\n",
    "        path = '../results/weightlearning/psl/performance_study'\n",
    "        datasets = [dataset for dataset in os.listdir(path) if os.path.isdir(os.path.join(path, dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jester']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../results/weightlearning/psl/performance_study'\n",
    "datasets = [dataset for dataset in os.listdir(path) if os.path.isdir(os.path.join(path, dataset))]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build single dataframe for each dataset\n",
    "for dataset_name in datasets.keys():\n",
    "    datasets[dataset_name][\"dataframe\"] = pd.DataFrame(index=range(datasets[dataset_name][\"n_folds\"]));\n",
    "    datasets[dataset_name][\"dataframe\"].index.name = 'fold'\n",
    "    for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "        for wl_method in weight_learning_methods:\n",
    "            results = pd.read_csv(dataset_name + '/' + dataset_name + \"-\" + wl_method + \"-eval-\" + evaluation_method + \".csv\", header=None).values[0]\n",
    "            results = results[:datasets[dataset_name]['n_folds']]\n",
    "            datasets[dataset_name][\"dataframe\"][evaluation_method + \"_\" + wl_method] = results\n",
    "\n",
    "columns=set()\n",
    "for dataset_name in datasets.keys():\n",
    "    for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "        columns.add(dataset_name + '_' + evaluation_method + '_mean')\n",
    "        columns.add(dataset_name + '_' + evaluation_method + '_standardDeviation')\n",
    "\n",
    "performance_dataframe = pd.DataFrame(index=weight_learning_methods, columns=columns)\n",
    "for wl_method in weight_learning_methods:\n",
    "    for dataset_name in datasets.keys():\n",
    "        for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "            results = pd.read_csv(dataset_name + '/' + dataset_name + \"-\" + wl_method + \"-eval-\" + evaluation_method + \".csv\", header=None).values[0]\n",
    "            results = results[:datasets[dataset_name]['n_folds']]\n",
    "            performance_dataframe.loc[wl_method, dataset_name + '_' + evaluation_method + '_mean'] = results.mean()\n",
    "            performance_dataframe.loc[wl_method, dataset_name + '_' + evaluation_method + '_standardDeviation'] = results.std()\n",
    "            \n",
    "performance_dataframe = performance_dataframe.reindex(sorted(performance_dataframe.columns), axis=1)\n",
    "performance_dataframe.to_csv(\"performance_study_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Citeseer_Accuracy_mean</th>\n",
       "      <th>Citeseer_Accuracy_standardDeviation</th>\n",
       "      <th>Citeseer_F1_mean</th>\n",
       "      <th>Citeseer_F1_standardDeviation</th>\n",
       "      <th>Cora_Accuracy_mean</th>\n",
       "      <th>Cora_Accuracy_standardDeviation</th>\n",
       "      <th>Cora_F1_mean</th>\n",
       "      <th>Cora_F1_standardDeviation</th>\n",
       "      <th>Epinions_AUROC_mean</th>\n",
       "      <th>Epinions_AUROC_standardDeviation</th>\n",
       "      <th>Epinions_F1_mean</th>\n",
       "      <th>Epinions_F1_standardDeviation</th>\n",
       "      <th>Jester_AUROC_mean</th>\n",
       "      <th>Jester_AUROC_standardDeviation</th>\n",
       "      <th>Jester_MSE_mean</th>\n",
       "      <th>Jester_MSE_standardDeviation</th>\n",
       "      <th>LastFM_AUROC_mean</th>\n",
       "      <th>LastFM_AUROC_standardDeviation</th>\n",
       "      <th>LastFM_MSE_mean</th>\n",
       "      <th>LastFM_MSE_standardDeviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RGS</th>\n",
       "      <td>0.741013</td>\n",
       "      <td>0.0117652</td>\n",
       "      <td>0.797617</td>\n",
       "      <td>0.00695118</td>\n",
       "      <td>0.83299</td>\n",
       "      <td>0.014448</td>\n",
       "      <td>0.866319</td>\n",
       "      <td>0.0148332</td>\n",
       "      <td>0.811908</td>\n",
       "      <td>0.0302242</td>\n",
       "      <td>0.964475</td>\n",
       "      <td>0.00568882</td>\n",
       "      <td>0.769814</td>\n",
       "      <td>0.00155714</td>\n",
       "      <td>0.0521525</td>\n",
       "      <td>0.000476229</td>\n",
       "      <td>0.598016</td>\n",
       "      <td>0.00459492</td>\n",
       "      <td>0.078146</td>\n",
       "      <td>0.000922336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRGS</th>\n",
       "      <td>0.744561</td>\n",
       "      <td>0.0106014</td>\n",
       "      <td>0.788449</td>\n",
       "      <td>0.00798026</td>\n",
       "      <td>0.833914</td>\n",
       "      <td>0.0176771</td>\n",
       "      <td>0.855506</td>\n",
       "      <td>0.0115167</td>\n",
       "      <td>0.814475</td>\n",
       "      <td>0.0217344</td>\n",
       "      <td>0.957941</td>\n",
       "      <td>0.00634273</td>\n",
       "      <td>0.771078</td>\n",
       "      <td>0.00151763</td>\n",
       "      <td>0.0552375</td>\n",
       "      <td>0.000269525</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>0.0018314</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>0.000832697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HB</th>\n",
       "      <td>0.743052</td>\n",
       "      <td>0.00880311</td>\n",
       "      <td>0.785321</td>\n",
       "      <td>0.00770918</td>\n",
       "      <td>0.836037</td>\n",
       "      <td>0.0152339</td>\n",
       "      <td>0.857889</td>\n",
       "      <td>0.0099019</td>\n",
       "      <td>0.808821</td>\n",
       "      <td>0.0224004</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.00581896</td>\n",
       "      <td>0.7709</td>\n",
       "      <td>0.00151919</td>\n",
       "      <td>0.0571412</td>\n",
       "      <td>0.000176028</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>0.0018314</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>0.000832697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Citeseer_Accuracy_mean Citeseer_Accuracy_standardDeviation  \\\n",
       "RGS                0.741013                           0.0117652   \n",
       "CRGS               0.744561                           0.0106014   \n",
       "HB                 0.743052                          0.00880311   \n",
       "\n",
       "     Citeseer_F1_mean Citeseer_F1_standardDeviation Cora_Accuracy_mean  \\\n",
       "RGS          0.797617                    0.00695118            0.83299   \n",
       "CRGS         0.788449                    0.00798026           0.833914   \n",
       "HB           0.785321                    0.00770918           0.836037   \n",
       "\n",
       "     Cora_Accuracy_standardDeviation Cora_F1_mean Cora_F1_standardDeviation  \\\n",
       "RGS                         0.014448     0.866319                 0.0148332   \n",
       "CRGS                       0.0176771     0.855506                 0.0115167   \n",
       "HB                         0.0152339     0.857889                 0.0099019   \n",
       "\n",
       "     Epinions_AUROC_mean Epinions_AUROC_standardDeviation Epinions_F1_mean  \\\n",
       "RGS             0.811908                        0.0302242         0.964475   \n",
       "CRGS            0.814475                        0.0217344         0.957941   \n",
       "HB              0.808821                        0.0224004         0.957346   \n",
       "\n",
       "     Epinions_F1_standardDeviation Jester_AUROC_mean  \\\n",
       "RGS                     0.00568882          0.769814   \n",
       "CRGS                    0.00634273          0.771078   \n",
       "HB                      0.00581896            0.7709   \n",
       "\n",
       "     Jester_AUROC_standardDeviation Jester_MSE_mean  \\\n",
       "RGS                      0.00155714       0.0521525   \n",
       "CRGS                     0.00151763       0.0552375   \n",
       "HB                       0.00151919       0.0571412   \n",
       "\n",
       "     Jester_MSE_standardDeviation LastFM_AUROC_mean  \\\n",
       "RGS                   0.000476229          0.598016   \n",
       "CRGS                  0.000269525          0.594382   \n",
       "HB                    0.000176028          0.594382   \n",
       "\n",
       "     LastFM_AUROC_standardDeviation LastFM_MSE_mean  \\\n",
       "RGS                      0.00459492        0.078146   \n",
       "CRGS                      0.0018314        0.091716   \n",
       "HB                        0.0018314        0.091716   \n",
       "\n",
       "     LastFM_MSE_standardDeviation  \n",
       "RGS                   0.000922336  \n",
       "CRGS                  0.000832697  \n",
       "HB                    0.000832697  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets.keys():\n",
    "    datasets[dataset_name][\"dataframe\"] = pd.DataFrame(index=range(datasets[dataset_name][\"n_folds\"]));\n",
    "    datasets[dataset_name][\"dataframe\"].index.name = 'fold'\n",
    "    for evaluation_method in datasets[dataset_name][\"evaluation_methods\"]:\n",
    "        for wl_method in weight_learning_methods:\n",
    "            results = pd.read_csv(dataset_name + '/' + dataset_name + \"-\" + wl_method + \"-eval-\" + evaluation_method + \".csv\", header=None).values[0]\n",
    "            results = results[:datasets[dataset_name]['n_folds']]\n",
    "            datasets[dataset_name][\"dataframe\"][evaluation_method + \"_\" + wl_method] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fcc001aecc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'Jester'\n",
    "datasets[dataset_name][\"dataframe\"].mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=set()\n",
    "for dataset_name in datasets.keys():\n",
    "    for column in datasets[dataset_name]['dataframe'].columns:\n",
    "        columns.add(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dataframe = pd.DataFrame(columns=columns)\n",
    "standard_deviation_dataframe = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets.keys():\n",
    "    mean_series = datasets[dataset_name]['dataframe'].mean()\n",
    "    mean_series.name = dataset_name\n",
    "    mean_dataframe = mean_dataframe.append(mean_series)\n",
    "    \n",
    "    standard_deviation_series = datasets[dataset_name]['dataframe'].std()\n",
    "    standard_deviation_series.name = dataset_name\n",
    "    standard_deviation_dataframe = standard_deviation_dataframe.append(standard_deviation_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_dataframe = standard_deviation_dataframe.join(mean_dataframe, lsuffix='_std', rsuffix='_mean')\n",
    "performance_dataframe = performance_dataframe.reindex(sorted(performance_dataframe.columns), axis=1)\n",
    "performance_dataframe.to_csv(\"performance_study_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RGS_MSE</th>\n",
       "      <th>CRGS_F1</th>\n",
       "      <th>HB_MSE</th>\n",
       "      <th>HB_F1</th>\n",
       "      <th>RGS_F1</th>\n",
       "      <th>RGS_Accuracy</th>\n",
       "      <th>HB_Accuracy</th>\n",
       "      <th>CRGS_MSE</th>\n",
       "      <th>CRGS_Accuracy</th>\n",
       "      <th>RGS_AUROC</th>\n",
       "      <th>HB_AUROC</th>\n",
       "      <th>CRGS_AUROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jester</th>\n",
       "      <td>0.052152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.769814</td>\n",
       "      <td>0.770900</td>\n",
       "      <td>0.771078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epinions</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.964475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.811908</td>\n",
       "      <td>0.808821</td>\n",
       "      <td>0.814475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cora</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.857889</td>\n",
       "      <td>0.866319</td>\n",
       "      <td>0.832990</td>\n",
       "      <td>0.836037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.833914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Citeseer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.788449</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.785321</td>\n",
       "      <td>0.797617</td>\n",
       "      <td>0.741013</td>\n",
       "      <td>0.743053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LastFM</th>\n",
       "      <td>0.078146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598016</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>0.594382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RGS_MSE   CRGS_F1    HB_MSE     HB_F1    RGS_F1  RGS_Accuracy  \\\n",
       "Jester    0.052152       NaN  0.057141       NaN       NaN           NaN   \n",
       "Epinions       NaN  0.957941       NaN  0.957346  0.964475           NaN   \n",
       "Cora           NaN  0.855506       NaN  0.857889  0.866319      0.832990   \n",
       "Citeseer       NaN  0.788449       NaN  0.785321  0.797617      0.741013   \n",
       "LastFM    0.078146       NaN  0.091716       NaN       NaN           NaN   \n",
       "\n",
       "          HB_Accuracy  CRGS_MSE  CRGS_Accuracy  RGS_AUROC  HB_AUROC  \\\n",
       "Jester            NaN  0.055238            NaN   0.769814  0.770900   \n",
       "Epinions          NaN       NaN            NaN   0.811908  0.808821   \n",
       "Cora         0.836037       NaN       0.833914        NaN       NaN   \n",
       "Citeseer     0.743053       NaN       0.744561        NaN       NaN   \n",
       "LastFM            NaN  0.091716            NaN   0.598016  0.594382   \n",
       "\n",
       "          CRGS_AUROC  \n",
       "Jester      0.771078  \n",
       "Epinions    0.814475  \n",
       "Cora             NaN  \n",
       "Citeseer         NaN  \n",
       "LastFM      0.594382  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '2'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[0,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f72541674636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "l.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
